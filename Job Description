Job Description:

1. 
I am a fourth-year Ph.D. student in the Department of EECS at UC Berkeley, where I am collaborating with Prof. Kannan Ramchandran and Prof. Michael Mahoney on devising fast algorithms for machine learning on the cloud. Our algorithms are inspired by ideas from Randomized Numerical Linear Algebra and Information and Coding theory.

2.
Worked with the AI Research team at Apple on devising algorithms for distributed training of Deep Neural Networks with the objective of ameliorating generalization drop due to large-batch training.


I am a Ph.D. student in the Department of Electrical Engineering and Computer Sciences at UC Berkeley. During my Ph.D., I have been working on schemes for accelerating distributed optimization on the Cloud. 

At DeepMind, I would love to work on topics related to large-scale/distributed machine learning. More specifically, my ideal internship would be spent on devising principled algorithms for accelerating distributed deep learning. Ideally, such algorithms would be based on strong theoretical/analytical foundations.

I have a good experience in training deep neural networks and like to keep myself updated with the state-of-the-art in the field of Deep Learning.

During the summer of 2019, I also worked with the AI Research team at Apple on devising algorithms for distributed training of Deep Neural Networks with the objective of ameliorating generalization drop due to large-batch training. The results of the internship are under review in a premier machine learning conference.